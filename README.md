# DeepLearning

## 神经网络的学习目的是找到使损失函数的值尽可能小的权重参数

### 神经网络的学习步骤

+ 步骤一：先从mini-batch中随机挑选一些数据，计算损失函数的值
+ 步骤二：为了减小损失函数的值，需要计算各个权重参数的积分
+ 步骤三：将权重参数沿梯度方向进行微小调整
+ 步骤四：重复步骤一二三

使用反向传播法时，乘法反向时要反转
通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）
初始权重参数设置不好的话，会出现梯度消失和表现力不足的问题
用作激活函数的函数最好具有关于零点对称的性质
Sigmoid函数使用Xavier初始值，Relu函数使用He初始值

### Batch Norm的优点

+ 可以使学习快速进行（增大学习率）
+ 不那么依赖权重初始值
+ 抑制过拟合（降低Dropout等的必要性）
  