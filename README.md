# DeepLearning

## 神经网络的学习目的是找到使损失函数的值尽可能小的权重参数

### 神经网络的学习步骤

+ 步骤一：先从mini-batch中随机挑选一些数据，计算损失函数的值
+ 步骤二：为了减小损失函数的值，需要计算各个权重参数的积分
+ 步骤三：将权重参数沿梯度方向进行微小调整
+ 步骤四：重复步骤一二三

使用反向传播法时，乘法反向时要反转
通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）
初始权重参数设置不好的话，会出现梯度消失和表现力不足的问题
用作激活函数的函数最好具有关于零点对称的性质
Sigmoid函数使用Xavier初始值，Relu函数使用He初始值

### Batch Norm的优点

+ 可以使学习快速进行（增大学习率）
+ 不那么依赖权重初始值
+ 抑制过拟合（降低Dropout等的必要性）
  
机器学习的目的是提高泛化能力
过拟合：只能拟合训练数据，不能拟合非训练数据

### 过拟合的原因

+ 模型有大量参数，表现力强
+ 训练数据少

### 抑制过拟合的方法

+ 权值衰减（L2范数：所有元素的平方相加的平方根），模型简单时
+ Dropout，模型复杂时，跟集成学习相关

训练数据用于学习，测试数据用于评估泛化能力

### 超参数最优化

+ 设定超参数的范围
+ 使用上一步采样到的超参数的值进行学习，通过验证数据评估识别精度（把epoch设置得很小）
+ 重复上述步骤，根据识别精度的结果，缩小超参数的范围
  
卷积运算：输入数据与滤波器相乘后相加
填充的目的：调整输出的大小
步幅：应用滤波器的位置间隔
池化：缩小高、长方向上的空间运算(有MAX池化和Average池化)
池化的窗口大小和步幅大小相等

### 池化的特征

+ 没有要学习的参数
+ 通道数不变
+ 对微小的位置变化有鲁棒性

卷积运算利用im2col来加速运算，最后reshape

im2col:将应用滤波器的区域和滤波器横向展开为一个一维数组，因此数据会有重复，消耗较大空间，但矩阵运算非常快

CNN可以有效读取图像中的某种特性
通过学习，滤波器变成了有规律的滤波器

卷积层的滤波器会提取边缘或斑块等原始信息，随着层数的加深，提取的信息（反映强烈的神经元）也越来越抽象，神经元从简单的形状向高级信息变化，响应的对象也在变化

### AlexNet和LeNet的区别

+ 激活函数使用ReLu函数
+ 使用进行局部正规化的LRN层
+ 使用Dropout
  
加深层的好处:减少网络参数的数量，使学习更高效
叠加小型滤波器来加深网络的好处时可以减少参数的数量，扩大感受野
VGG:基于3X3的小型滤波器的卷积层运算是连续进行的，如先卷积层重叠2-4次，再通过池化层将大小减半，最后经由全连接层输出结果
GoogLeNet:在纵向上有深度，在横向上也有深度
ResNet：网络更深，加入了快捷结构，反向传播时可以无衰减的传播
CPU擅长连续复杂的运算，GPU擅长大型矩阵的乘积运算（通过im2col）
加快学习速度：GPU加速、分布式学习
神经网络具有健壮性，对精度要求不高
多模态处理：将组合图像和自然语言等多种信息进行处理
机器学习分为监督学习和无监督学习

### 深度学习的未来

+ 图像风格变换(2合1)
+ 图像生成（0合1)
+ 自动驾驶
+ 强化学习（自主学习，无评价）

人工智能包括机器学习，机器学习包括深度学习

```mermaid
graph LR
A[规则] -->C[经典的程序设计]
B[数据] -->C[经典的程序设计]
    C -->D[答案]
```

```mermaid
graph LR 
A[答案] -->C[机器学习]
B[数据] -->C[机器学习]
    C -->D[规则]
```

一个数：标量张量
一个数列：向量或1D张量，数字的个数叫维度
张量维度看最多括号数

分类问题时，维度较小可能会成为信息瓶颈，永久地丢失相关信息
标签向量化有两种方法，需要对应不同的损失函数
输出维度应大于等于隐藏单元个数
回归（连续）与分类(离散)
不能从测试数据上计算数据标准化的均差和标准差
一般来说，训练数据越少，过拟合越严重，较小的网络可以抑制过拟合
数据较少时，可以进行k折验证
回归常用的损失函数位均方误差（mse），常见的回归指标是平均绝对误差（mae），精度不适合回归问题
二分类使用sigmoid函数，损失函数使用binary_crossentropy
一般采用rmsprop优化器
对于单标签，多分类问题使用softmax，损失函数使用分类交叉熵，中间层不应太小
