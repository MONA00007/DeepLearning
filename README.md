# DeepLearning

## 神经网络的学习目的是找到使损失函数的值尽可能小的权重参数

### 神经网络的学习步骤

+ 步骤一：先从mini-batch中随机挑选一些数据，计算损失函数的值
+ 步骤二：为了减小损失函数的值，需要计算各个权重参数的积分
+ 步骤三：将权重参数沿梯度方向进行微小调整
+ 步骤四：重复步骤一二三

使用反向传播法时，乘法反向时要反转
通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）
初始权重参数设置不好的话，会出现梯度消失和表现力不足的问题
用作激活函数的函数最好具有关于零点对称的性质
Sigmoid函数使用Xavier初始值，Relu函数使用He初始值

### Batch Norm的优点

+ 可以使学习快速进行（增大学习率）
+ 不那么依赖权重初始值
+ 抑制过拟合（降低Dropout等的必要性）
  
机器学习的目的是提高泛化能力
过拟合：只能拟合训练数据，不能拟合非训练数据

### 过拟合的原因

+ 模型有大量参数，表现力强
+ 训练数据少

### 抑制过拟合的方法

+ 权值衰减（L2范数：所有元素的平方相加的平方根），模型简单时
+ Dropout，模型复杂时，跟集成学习相关

训练数据用于学习，测试数据用于评估泛化能力

### 超参数最优化

+ 设定超参数的范围
+ 使用上一步采样到的超参数的值进行学习，通过验证数据评估识别精度（把epoch设置得很小）
+ 重复上述步骤，根据识别精度的结果，缩小超参数的范围
  
卷积运算：输入数据与滤波器相乘后相加
填充的目的：调整输出的大小
步幅：应用滤波器的位置间隔
池化：缩小高、长方向上的空间运算(有MAX池化和Average池化)
池化的窗口大小和步幅大小相等

### 池化的特征

+ 没有要学习的参数
+ 通道数不变
+ 对微小的位置变化有鲁棒性

卷积运算利用im2col来加速运算，最后reshape
im2col:将应用滤波器的区域和滤波器横向展开为一个一维数组，因此数据会有重复，消耗较大空间，但矩阵运算非常快

